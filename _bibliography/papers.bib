---
---


@Article{s22020494,
AUTHOR = {McGowan, Erin and Gawade, Vidita and Guo, Weihong (Grace)},
TITLE = {A Physics-Informed Convolutional Neural Network with Custom Loss Functions for Porosity Prediction in Laser Metal Deposition},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {494},
URL = {https://www.mdpi.com/1424-8220/22/2/494},
PubMedID = {35062455},
ISSN = {1424-8220},
ABSTRACT = {Physics-informed machine learning is emerging through vast methodologies and in various applications. This paper discovers physics-based custom loss functions as an implementable solution to additive manufacturing (AM). Specifically, laser metal deposition (LMD) is an AM process where a laser beam melts deposited powder, and the dissolved particles fuse to produce metal components. Porosity, or small cavities that form in this printed structure, is generally considered one of the most destructive defects in metal AM. Traditionally, computer tomography scans measure porosity. While this is useful for understanding the nature of pore formation and its characteristics, purely physics-driven models lack real-time prediction ability. Meanwhile, a purely deep learning approach to porosity prediction leaves valuable physics knowledge behind. In this paper, a hybrid model that uses both empirical and simulated LMD data is created to show how various physics-informed loss functions impact the accuracy, precision, and recall of a baseline deep learning model for porosity prediction. In particular, some versions of the physics-informed model can improve the precision of the baseline deep learning-only model (albeit at the expense of overall accuracy).},
DOI = {10.3390/s22020494},
Note={Full text: https://www.mdpi.com/1424-8220/22/2/494},
preview={sensors_preview.webp}
}

@Article{pmid37131675,
   Author="McGowan, E.  and Sanjak, J.  and Math, E. A.  and Zhu, Q. ",
   Title="{{I}ntegrative {R}are {D}isease {B}iomedical {P}rofile based {N}etwork {S}upporting {D}rug {R}epurposing, a case study of {G}lioblastoma}",
   Journal="Res Sq",
   Year="2023",
   Month="Aug",
   Abstract={Background. Glioblastoma (GBM) is the most aggressive and common malignant primary brain tumor; however, treatment remains a significant challenge. This study aims to identify drug repurposing candidates for GBM by developing an integrative rare disease profile network containing heterogeneous types of biomedical data. Methods. We developed a Glioblastoma-based Biomedical Profile Network (GBPN) by extracting and integrating biomedical information pertinent to GBM-related diseases from the NCATS GARD Knowledge Graph (NGKG). We further clustered the GBPN based on modularity classes which resulted in multiple focused subgraphs, named mc_GBPN. We then identified high-influence nodes by performing network analysis over the mc_GBPN and validated those nodes that could be potential drug repositioning candidates for GBM. Results. We developed the GBPN with 1,466 nodes and 107,423 edges and consequently the mc_GBPN with forty-one modularity classes. A list of the ten most influential nodes were identified from the mc_GBPN. These notably include Riluzole, stem cell therapy, cannabidiol, and VK-0214, with proven evidence for treating GBM. Conclusion. Our GBM-targeted network analysis allowed us to effectively identify potential candidates for drug repurposing. This could lead to less invasive treatments for glioblastoma while significantly reducing research costs by shortening the drug development timeline. Furthermore, this workflow can be extended to other disease areas.},
   Note={Full text (preprint): https://pubmed.ncbi.nlm.nih.gov/37131675/},
   preview={GBM_preview.jpeg}
}

@misc{castelo2023argus,
      title={ARGUS: Visualization of AI-Assisted Task Guidance in AR}, 
      author={Sonia Castelo and Joao Rulff and Erin McGowan and Bea Steers and Guande Wu and Shaoyu Chen and Iran Roman and Roque Lopez and Ethan Brewer and Chen Zhao and Jing Qian and Kyunghyun Cho and He He and Qi Sun and Huy Vo and Juan Bello and Michael Krone and Claudio Silva},
      year={2023},
      Month="Aug",
      eprint={2308.06246},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      Abstract={The concept of augmented reality (AR) assistants has captured the human imagination for decades, becoming a staple of modern science fiction. To pursue this goal, it is necessary to develop artificial intelligence (AI)-based methods that simultaneously perceive the 3D environment, reason about physical tasks, and model the performer, all in real-time. Within this framework, a wide variety of sensors are needed to generate data across different modalities, such as audio, video, depth, speech, and time-of-flight. The required sensors are typically part of the AR headset, providing performer sensing and interaction through visual, audio, and haptic feedback. AI assistants not only record the performer as they perform activities, but also require machine learning (ML) models to understand and assist the performer as they interact with the physical world. Therefore, developing such assistants is a challenging task. We propose ARGUS, a visual analytics system to support the development of intelligent AR assistants. Our system was designed as part of a multi year-long collaboration between visualization researchers and ML and AR experts. This co-design process has led to advances in the visualization of ML in AR. Our system allows for online visualization of object, action, and step detection as well as offline analysis of previously recorded AR sessions. It visualizes not only the multimodal sensor data streams but also the output of the ML models. This allows developers to gain insights into the performer activities as well as the ML models, helping them troubleshoot, improve, and fine tune the components of the AR assistant.},
      Note={Full text (preprint): https://arxiv.org/abs/2308.06246},
      preview={teaser_v2.png}
}


